if (!require(stringr)) {install.packages("stringr")}
if (!require(dplyr)) {install.packages("dplyr")}
if (!require(tidytext)) {install.packages("tidytext")}
if (!require(tidyr)) {install.packages("tidyr")}
if (!require(ggplot2)) {install.packages("ggplot2")}
if (!require(wordcloud)) {install.packages("wordcloud")}
if (!require(tm)) {install.packages("tm")}
if (!require(quanteda)) {install.packages("quanteda")}

run_kmeans_scree <- function(mydata, max_clus=15){
  
  set.seed(seed = 0000)   # set seed for reproducible work
  wss <- (nrow(mydata)-1)*sum(apply(mydata,2,var))  # wss is within group sum of squares
  
  for (i in 2:max_clus) wss[i] <- sum(      # checking model fit for 2 to 15 clusters
    kmeans(mydata,  centers = i)$withinss)  # note use of kmeans() func
  
  # windows()    # opens a new window for plots
  plot(1:max_clus, wss, type="b", 
       xlab="Number of Clusters",
       ylab="Within groups sum of squares")
  
}  # func ends

text.clean = function(x){                    # x = text data
  
  require("tm")
  x  =  gsub("<.*?>", " ", x)               # regex for removing HTML tags
  x  =  iconv(x, "latin1", "ASCII", sub="") # Keep only ASCII characters
  x  =  gsub("[^[:alnum:]]", " ", x)        # keep only alpha numeric 
  x  =  tolower(x)                          # convert to lower case characters
  #  x  =  removeNumbers(x)                    # removing numbers
  x  =  stripWhitespace(x)                  # removing white space
  x  =  gsub("^\\s+|\\s+$", "", x)          # remove leading and trailing white space
  
  # Read Stopwords list
  stpw1 = readLines('https://raw.githubusercontent.com/sudhir-voleti/basic-text-analysis-shinyapp/master/data/stopwords.txt')# stopwords list
  stpw2 = tm::stopwords('english')      # tm package stop word list; tokenizer package has the same name function, hence 'tm::'
  comn  = unique(c(stpw1, stpw2))         # Union of two list
  stopwords = unique(gsub("'"," ",comn))  # final stop word lsit after removing punctuation
  
  x  =  removeWords(x,stopwords)            # removing stopwords created above
  x  =  stripWhitespace(x)                  # removing white space
  #  x  =  stemDocument(x)                   # can stem doc if needed.
  
  return(x)
}

dtm.tcm.creator <- function(text,
                            id = "",
                            std.clean = TRUE,
                            #                          std.stop.words = TRUE,
                            #                          stop.words.additional = c('a','b'),
                            bigram.encoding = TRUE,
                            bigram.min.freq = 2,
                            min.dtm.freq = 2,
                            skip.grams.window = 5) {    # for TCM local window half-length
  
  if (class(text) != "character" | length(text) < 3){
    stop("data format Not correct. Make sure it's a character verctor of length above 3")
  }
  
  if ((id == "")[1]){
    id = 1:length(text)
  }
  
  require(tm)
  if (std.clean == TRUE) {
    print("Performing Standard Text Cleaning")
    
    text = text.clean(text)
  }
  
  #    if (std.stop.words == TRUE){
  #      print("Removing Stop Words")
  #      stop.words.f = unique(c(stpw3,stop.words.additional))
  #      text = removeWords(text,stop.words.f)            # removing stopwords created above
  #      text = stripWhitespace(text)                  # removing white spacestop.words.additional
  #    }
  
  require(text2vec)
  tok_fun = word_tokenizer  # using word & not space tokenizers
  
  if (bigram.encoding == TRUE){
    
    # data = data.frame(id = 1:length(text),text = text, stringsAsFactors = F)
    
    print("finding bi-grams for encoding with selected criteria")
    
    it_0 = itoken( text,
                   tokenizer = tok_fun,
                   ids = id,
                   progressbar = T)
    
    vocab = create_vocabulary(it_0, ngram = c(2L, 2L))
    pruned_vocab = prune_vocabulary(vocab, term_count_min = bigram.min.freq)
    replace_list = pruned_vocab$vocab$terms[order(pruned_vocab$vocab$terms_counts, decreasing = T)]
    
    if (length(replace_list) > 0){
      text = paste("",text,"")
      
      pb <- txtProgressBar(min = 1, max = (length(replace_list)), style = 3) ; i = 0
      
      print(paste("Encoding",length(replace_list),"bi-grams as unigram"))
      for (term in replace_list){
        i = i + 1
        focal.term = gsub("_", " ",term)        # in case dot was word-separator
        replacement.term = term
        text = gsub(paste("",focal.term,""),paste("",replacement.term,""), text)
        setTxtProgressBar(pb, i)
      }                  
    } else {
      print("No bigram to encode with selected criteria")}
  }
  
  print("Creating Document Term Matrix")
  # Create DTM
  it_m = itoken(text,
                tokenizer = tok_fun,
                ids = id,
                progressbar = T)
  
  vocab = create_vocabulary(it_m)
  pruned_vocab = prune_vocabulary(vocab,
                                  term_count_min = min.dtm.freq)
  
  vectorizer = vocab_vectorizer(pruned_vocab)
  
  dtm_m  = create_dtm(it_m, vectorizer)
  dtm = as.DocumentTermMatrix(dtm_m, weighting = weightTf)
  
  print("Creating Term Co-occurrence Matrix")
  
  vectorizer = vocab_vectorizer(pruned_vocab,
                                grow_dtm = FALSE,
                                skip_grams_window = skip.grams.window)
  
  tcm = create_tcm(it_m, vectorizer) # func to build a TCM
  
  print("Done!!")
  out = list(dtm = dtm, tcm = tcm)
  
  return(out)
}

dtm.word.count <- function(dtm) {
  
  if (ncol(dtm) > 1000) {
    tst = round(ncol(dtm)/100)  # divide DTM's cols into 100 manageble parts
    a = rep(tst,99)
    b = cumsum(a);rm(a)
    b = b[-which(b >= ncol(dtm))]
    b = c(0,b,ncol(dtm))
    
    ss.col = c(NULL)
    for (i in 1:(length(b)-1)) {
      tempdtm = dtm[,(b[i]+1):(b[i+1])]
      s = colSums(as.matrix(tempdtm))
      ss.col = c(ss.col,s)
    }
  } else {
    ss.col = colSums(as.matrix(dtm))
  }
  
  tsum = ss.col
  tsum = tsum[order(tsum, decreasing = T)]       #terms in decreasing order of freq
  return(tsum)
}

dtm.word.cloud <- function(count = count, title = "Title", max_words){
  
  require(wordcloud)
  
  if (class(count)[1] == "DocumentTermMatrix"|class(count)[1] == "simple_triplet_matrix")
  {
    tsum = dtm.word.count(count)
  } else {
    tsum = count
  }
  
  if (class(tsum) != "numeric") stop("Give input as wordcount or DocumentTermMatrix")
  
  wordcloud(names(tsum), tsum,     # words, their freqs 
            scale = c(4, 0.5),     # range of word sizes
            1,                     # min.freq of words to consider
            max.words = max_words,       # max #words
            colors = brewer.pal(8, "Dark2"))    # Plot results in a word cloud 
  title(sub = title)     # title for the wordcloud display
}   

distill.cog.tcm = function(mat1, # input TCM or DTM MAT
                           title, # title for the graph
                           s,    # no. of central nodes
                           k1){  # max no. of connections  
  require(igraph)
  
  mat1 = as.matrix(mat1)
  mat1 = t(mat1) %*% mat1
  
  
  if (ncol(mat1) > 1000) {
    tst = round(ncol(mat1)/100)  # divide mat1's cols into 100 manageble parts
    a = rep(tst,99)
    b = cumsum(a);rm(a)
    b = b[-which(b >= ncol(mat1))]
    b = c(0,b,ncol(mat1))
    
    ss.col = c(NULL)
    for (i in 1:(length(b)-1)) {
      tempmat1 = mat1[,(b[i]+1):(b[i+1])]
      su = colSums(as.matrix(tempmat1))
      ss.col = c(ss.col,su);rm(su)
    }
  } else {
    ss.col = colSums(as.matrix(mat1))
  }
  
  # a = colSums(mat1) # collect colsums into a vector obj a
  a = ss.col
  b = order(-a)     # nice syntax for ordering vector in decr order  
  
  mat2 = mat1[b, b]     # order both rows and columns along vector b
  
  diag(mat2) =  0
  
  ## +++ go row by row and find top k adjacencies +++ ##
  
  wc = NULL
  
  for (i1 in 1:s){ 
    thresh1 = mat2[i1,][order(-mat2[i1, ])[k1]]
    mat2[i1, mat2[i1,] < thresh1] = 0   # neat. didn't need 2 use () in the subset here.
    mat2[i1, mat2[i1,] > 0 ] = 1
    word = names(mat2[i1, mat2[i1,] > 0])
    mat2[(i1+1):nrow(mat2), match(word,colnames(mat2))] = 0
    wc = c(wc,word)
  } # i1 loop ends
  
  mat3 = mat2[match(wc, colnames(mat2)), match(wc, colnames(mat2))]
  ord = colnames(mat2)[which(!is.na(match(colnames(mat2), colnames(mat3))))]  # removed any NAs from the list
  mat4 = mat3[match(ord, colnames(mat3)), match(ord, colnames(mat3))]
  
  graph <- graph.adjacency(mat4, mode = "undirected", weighted=T)    # Create Network object
  
  
  graph = simplify(graph) 
  V(graph)$color[1:s] = "green"
  V(graph)$color[(s+1):length(V(graph))] = "pink"
  
  graph = delete.vertices(graph, V(graph)[ degree(graph) == 0 ]) # delete singletons?
  
  plot(graph, 
       layout = layout.kamada.kawai, 
       main = title)
} 


# if (!require(tm)) {install.packages("tm")}

clean_text <- function(x)
{
  input_text <- x
  input_text = paste(input_text, " ")
  input_text  =  iconv(input_text, "latin1", "ASCII", sub="")
  input_text  =  gsub("<.*?>", " ", input_text)
  input_text =  gsub("[^[:alnum:]]", " ", input_text)
  input_text =  tolower(input_text)
  input_text  =  removeNumbers(input_text)
  input_text  =  stripWhitespace(input_text)
  input_text  =  gsub("^\\s+|\\s+$", "", input_text)
  
  input_text <- unlist(input_text, recursive = T)
  indexes <- which(input_text == "")
  if(length(indexes) > 0)
  {
    input_text <- input_text[-indexes]
  }
  
  return(input_text)
}

remove_stopwords <- function(input, custom_stopwords)
{
  require(tm)
  require(tidytext)
  
  uniqe_stopwords = unique(c(custom_stopwords, stop_words$word))
  output <- removeWords(input, uniqe_stopwords) %>% stripWhitespace
  
  return(output)
}


get_sentiment_df <- function(textdf)
{
  require(tidytext)
  nrc = get_sentiments("nrc")   # put all of the nrc sentiment dict into object 'nrc'
  
  senti.nrc = textdf %>%
    mutate(linenumber = row_number()) %>%
    ungroup() %>%
    unnest_tokens(word, text) %>%
    inner_join(get_sentiments("nrc")) %>%
    count(sentiment, index = linenumber %/% 1, sort = FALSE) %>%  # %/% gives quotient
    mutate(method = "nrc")
  
  nrc_df = data.frame(senti.nrc %>% spread(sentiment, n, fill = 0))
  
  return(nrc_df)
}

top_joy_docs <- function(nrc_df, n)
{
  top_sentiments_nrc = rev(order(nrc_df$joy))[1:n]
  top_sentiment_texts <- data.frame(nrc_df[top_sentiments_nrc,]$index, nrc_df[top_sentiments_nrc,]$joy)
  colnames(top_sentiment_texts) <- c("index","freq")
  
  return(top_sentiment_texts)
}

top_anger_docs <- function(nrc_df, n)
{
  top_sentiments_nrc = rev(order(nrc_df$anger))[1:n]
  top_sentiment_texts <- data.frame(nrc_df[top_sentiments_nrc,]$index, nrc_df[top_sentiments_nrc,]$anger)
  colnames(top_sentiment_texts) <- c("index","freq")
  
  return(top_sentiment_texts)
}

top_anticipation_docs <- function(nrc_df, n)
{
  top_sentiments_nrc = rev(order(nrc_df$anticipation))[1:n]
  top_sentiment_texts <- data.frame(nrc_df[top_sentiments_nrc,]$index, nrc_df[top_sentiments_nrc,]$anticipation)
  colnames(top_sentiment_texts) <- c("index","freq")
  
  return(top_sentiment_texts)
}
